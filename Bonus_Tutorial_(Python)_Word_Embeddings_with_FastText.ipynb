{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bonus Tutorial (Python): Word Embeddings with FastText",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V0jTWdSFY-i"
      },
      "source": [
        "# Data \n",
        "\n",
        "This notebook illustrates the estimation of embeddings on a corpus of Supreme Court oral arguments. The data are available via the excellent Cornell Conversational Analysis Toolkit (ConvoKit). You can read more about the data (and ConvoKit) [here](https://convokit.cornell.edu/documentation/supreme.html#).  The Oral Arguments corpus is described as:\n",
        "\n",
        ">A collection of cases from the U.S. Supreme Court, along with transcripts of oral arguments. Contains approximately 1,700,000 utterances over 8,000 oral arguments transcripts from 7,700 cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtiK8ZXLR6o2",
        "outputId": "8e4ebef9-8c6a-44db-b877-d4f49aaf6911"
      },
      "source": [
        "!pip3 install convokit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting convokit\n",
            "  Downloading convokit-2.5.3.tar.gz (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from convokit) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.7/dist-packages (from convokit) (1.3.5)\n",
            "Collecting msgpack-numpy>=0.4.3.2\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.7/dist-packages (from convokit) (3.3.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from convokit) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from convokit) (1.0.2)\n",
            "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.7/dist-packages (from convokit) (3.7)\n",
            "Requirement already satisfied: dill>=0.2.9 in /usr/local/lib/python3.7/dist-packages (from convokit) (0.3.5.1)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from convokit) (1.1.0)\n",
            "Collecting clean-text>=0.1.1\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting unidecode>=1.1.1\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 49.7 MB/s \n",
            "\u001b[?25hCollecting emoji<2.0.0,>=1.0.0\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 10.3 MB/s \n",
            "\u001b[?25hCollecting ftfy<7.0,>=6.0\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 613 kB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.1.1->convokit) (0.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.3)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->convokit) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->convokit) (4.1.1)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4->convokit) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4->convokit) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.4->convokit) (4.64.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.4->convokit) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.0.0->convokit) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->convokit) (3.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (1.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (0.9.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (3.3.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (0.7.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (2.4.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (8.0.17)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (2.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (3.0.6)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (0.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.3.5->convokit) (1.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=2.3.5->convokit) (3.8.0)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.3.5->convokit) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.3.5->convokit) (2.0.1)\n",
            "Building wheels for collected packages: convokit, emoji\n",
            "  Building wheel for convokit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for convokit: filename=convokit-2.5.3-py3-none-any.whl size=204129 sha256=debe534ccd39851341f22c91a03469a60b097294a50a07781edc747a869c3e11\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/e8/2d/81c4477fe586fe4dad2de2886b990e90e839ffccd5158ed0f3\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=4bc5323895baa0c8e01a27f9507a62e0839ef874f02f82f2c9517d3cee8d8b36\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built convokit emoji\n",
            "Installing collected packages: ftfy, emoji, unidecode, msgpack-numpy, clean-text, convokit\n",
            "Successfully installed clean-text-0.6.0 convokit-2.5.3 emoji-1.7.0 ftfy-6.1.1 msgpack-numpy-0.4.8 unidecode-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIDud31zMirS"
      },
      "source": [
        "## Prepping the Corpus\n",
        "\n",
        "The first thing we need to do is to download the corpus. This will take a couple minutes, as this is a large corpus. Lawyers and judges like to talk a lot. The benefit of this additional text, though, is that we have significantly more information for validly estimating the word embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-2H6qf2SRxL"
      },
      "source": [
        "from convokit import Corpus, download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWHShYy_S87X",
        "outputId": "c3d9afea-18b9-4971-e88f-4a09dd2e5535"
      },
      "source": [
        "corpus = Corpus(filename=download(\"supreme-corpus\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading supreme-corpus to /root/.convokit/downloads/supreme-corpus\n",
            "Downloading supreme-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/supreme-corpus/supreme-corpus.zip (1255.8MB)... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see a bit of information on our corpus as follows. "
      ],
      "metadata": {
        "id": "0mK_mjx1VlZs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qH4C22lTN-e",
        "outputId": "5aad7209-e587-4aca-f4b8-971b8b28f89f"
      },
      "source": [
        "corpus.print_summary_stats()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Speakers: 8979\n",
            "Number of Utterances: 1700789\n",
            "Number of Conversations: 7817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTBmXmG9Mwcw"
      },
      "source": [
        "Let's look at the first utterance. This is the Chief Justice of the U.S. Supreme Court introducing the case and the first lawyer to speak before the Court."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flNbncFyT5m6",
        "outputId": "07c4e436-483a-4098-c477-bf5e3c32afd9"
      },
      "source": [
        "for utt in corpus.iter_utterances():\n",
        "    print(utt.text)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number 71, Lonnie Affronti versus United States of America.\n",
            "Mr. Murphy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-XNgRitMovA"
      },
      "source": [
        "Next, we need to begin to prepare the corpus for estimating word embeddings. To do so, we must first do some standard NLP tasks, segmenting the corpus by sentence and tokenizing the texts. We'll just use the nltk tokenizers to segment into sentences and tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP-wIrM4VKM7",
        "outputId": "522804fc-9523-40ba-cbee-51b61ab339d1"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp_8lT46Mzj1"
      },
      "source": [
        "Let's look at how the tokenizer works for the first utterance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsH8k9vF5LQz",
        "outputId": "458058e5-2110-4639-c3e8-228b8c985a7d"
      },
      "source": [
        "for utt in corpus.iter_utterances():\n",
        "    print( [word_tokenize(t) for t in sent_tokenize(utt.text)])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Number', '71', ',', 'Lonnie', 'Affronti', 'versus', 'United', 'States', 'of', 'America', '.'], ['Mr.', 'Murphy', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUrQAcZ6U6Bw"
      },
      "source": [
        "Generate the sentence tokens, and the word tokens within them. This took ~ 11 minutes, given 1.7 million utterances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEEW9D1C9NAo"
      },
      "source": [
        "sents = []\n",
        "for utt in corpus.iter_utterances():\n",
        "    sents.append([word_tokenize(t) for t in sent_tokenize(utt.text)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-THHCld6odo",
        "outputId": "d28ea359-6956-4e15-ac2d-ff3b1fbb4804"
      },
      "source": [
        "len(sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1700789"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGSiRF8xHzuw",
        "outputId": "261a658d-5076-4174-958a-c60f088c978f"
      },
      "source": [
        "sents[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['May', 'it', 'please', 'the', 'Court', '.'],\n",
              " ['We',\n",
              "  'are',\n",
              "  'here',\n",
              "  'by',\n",
              "  'writ',\n",
              "  'of',\n",
              "  'certiorari',\n",
              "  'to',\n",
              "  'the',\n",
              "  'Eighth',\n",
              "  'Circuit',\n",
              "  '.'],\n",
              " ['There',\n",
              "  'is',\n",
              "  'one',\n",
              "  'question',\n",
              "  'to',\n",
              "  'be',\n",
              "  'decided',\n",
              "  'in',\n",
              "  'this',\n",
              "  'case',\n",
              "  ',',\n",
              "  'decided',\n",
              "  'carefully',\n",
              "  '.'],\n",
              " ['Upon',\n",
              "  'sentence',\n",
              "  'to',\n",
              "  'consecutive',\n",
              "  'sentences',\n",
              "  'or',\n",
              "  'terms',\n",
              "  'by',\n",
              "  'a',\n",
              "  'District',\n",
              "  'Court',\n",
              "  '.'],\n",
              " ['The',\n",
              "  'defending',\n",
              "  'pattern',\n",
              "  'started',\n",
              "  'the',\n",
              "  'service',\n",
              "  'of',\n",
              "  'a',\n",
              "  'first',\n",
              "  'sentence',\n",
              "  '.'],\n",
              " ['Thus',\n",
              "  ',',\n",
              "  'the',\n",
              "  'District',\n",
              "  'Court',\n",
              "  'thereafter',\n",
              "  'have',\n",
              "  'jurisdiction',\n",
              "  'to',\n",
              "  'suspend',\n",
              "  'the',\n",
              "  'execution',\n",
              "  'of',\n",
              "  'the',\n",
              "  'remaining',\n",
              "  'sentences',\n",
              "  'and',\n",
              "  'place',\n",
              "  'the',\n",
              "  'defendant',\n",
              "  'on',\n",
              "  'probation',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxTHaAgPVNho"
      },
      "source": [
        "That's the second document/utterance, a list of lists (each sentence is a list of tokens). That means sents is organized as a list of lists of lists. The model wants a list of lists (the tokens by sentence, without distinguishing between the utterances in which they are used). So, we flatten the list (to a list of sentences, each a list of tokens)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqZrugUSN5cD"
      },
      "source": [
        "flat_sents_list = [sentence for utt in sents for sentence in utt] # for every utterance, loop over its sentences and add them to the list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fi0uuGzW4fh",
        "outputId": "7fbf0974-0d14-4274-b7d9-322a53689de9"
      },
      "source": [
        "len(flat_sents_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3880254"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxu2thVkqslh"
      },
      "source": [
        "As you can see, we are closing in on 4 million sentences overall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL5JDR3LOjv6"
      },
      "source": [
        "## FastText\n",
        "\n",
        "FastText embeddings (takes 15 min)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW8cbVrzHFeQ"
      },
      "source": [
        "from gensim.models import FastText\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4O2LmAnHbAF"
      },
      "source": [
        "modelf_w5 = FastText(sentences=flat_sents_list, size=100, window=5, min_count=5, workers=1)\n",
        "modelf_w5.save(\"w5_fasttext.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVN-oYwjLaQC",
        "outputId": "aa072582-1b5f-484b-de85-a64a462744df"
      },
      "source": [
        "vectors_w5_f = np.asarray(modelf_w5.wv.vectors)\n",
        "labels_w5_f = np.asarray(modelf_w5.wv.index2word)\n",
        "\n",
        "kmeans_w5_f_20 = KMeans(n_clusters=20)\n",
        "kmeans_w5_f_20.fit(vectors_w5_f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
              "       n_clusters=20, n_init=10, n_jobs=None, precompute_distances='auto',\n",
              "       random_state=None, tol=0.0001, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd6uc4UHMHTI",
        "outputId": "d8c0bb57-80bf-469c-d15a-982762325a8a"
      },
      "source": [
        "for k in range(20):\n",
        "  print(modelf_w5.wv.most_similar([kmeans_w5_f_20.cluster_centers_[k]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('acknowledgment', 0.8484030961990356), ('concede', 0.8400231003761292), ('reappoint', 0.8161632418632507), ('acknowledgement', 0.8080217242240906), ('Vatersay', 0.8050147294998169), ('appreciate', 0.8028841018676758), ('understand—and', 0.8028236627578735), ('criticise', 0.7989882230758667), ('reassert', 0.7972162961959839), ('conquest', 0.7933142185211182)]\n",
            "[('Could', 0.8523712158203125), ('is—will', 0.8343628644943237), ('Gould', 0.8303952813148499), ('will—will', 0.827309787273407), ('—will', 0.8239887356758118), ('will', 0.8192586898803711), ('do—will', 0.8120394945144653), ('Would', 0.8107205629348755), ('Should', 0.8085587024688721), ('Will', 0.7992162704467773)]\n",
            "[('alignment', 0.9004708528518677), ('propulsion', 0.8757753968238831), ('torment', 0.8706355094909668), ('destination', 0.8695518970489502), ('temperament', 0.8676959276199341), ('monument', 0.8650738000869751), ('apportionment', 0.8616500496864319), ('clandestinely', 0.8612180352210999), ('intimation', 0.8609365820884705), ('internment', 0.860916256904602)]\n",
            "[('172,000', 0.9964447021484375), ('152,000', 0.996036946773529), ('405,000', 0.9957195520401001), ('42,000', 0.995684027671814), ('176,000', 0.9955022931098938), ('135,000', 0.9954720139503479), ('155,000', 0.9954310059547424), ('266,000', 0.9952174425125122), ('92,000', 0.9951858520507812), ('422,000', 0.9950079321861267)]\n",
            "[('179', 0.9482035636901855), ('1701', 0.9467232823371887), ('110', 0.9464747905731201), ('283', 0.9462747573852539), ('515', 0.9451336860656738), ('115', 0.9445115327835083), ('310', 0.9441789388656616), ('174', 0.9423938989639282), ('177', 0.942257285118103), ('5.15', 0.9408314228057861)]\n",
            "[('Tristar', 0.7772427797317505), ('Elephant', 0.7691064476966858), ('Jewry', 0.7612568140029907), ('Vilnius', 0.7586961388587952), ('Orissa', 0.7579789161682129), ('Gaulle', 0.7540204524993896), ('Fergus', 0.7526495456695557), ('Gamble', 0.7520689964294434), ('Qatar', 0.7471625804901123), ('Quayle', 0.7461229562759399)]\n",
            "[('ding', 0.977282702922821), ('skating', 0.9753019213676453), ('axing', 0.9723691940307617), ('Ging', 0.9720653295516968), ('Ayling', 0.9717262983322144), ('bowling', 0.971550464630127), ('Ring', 0.970129132270813), ('boating', 0.9701200127601624), ('ing', 0.9695068001747131), ('guzzling', 0.9678100943565369)]\n",
            "[('intangible', 0.9205235242843628), ('interoperable', 0.9073357582092285), ('cogent', 0.9065909385681152), ('interchangeable', 0.8985979557037354), ('propitious', 0.896164059638977), ('colorectal', 0.8960276246070862), ('salient', 0.8950022459030151), ('primitive', 0.8913304209709167), ('inimitable', 0.8913168907165527), ('indefatigable', 0.8912927508354187)]\n",
            "[('fated', 0.9755768179893494), ('bolted', 0.9705144762992859), ('bloated', 0.9699353575706482), ('shied', 0.9679064750671387), ('bated', 0.9675759077072144), ('redacted', 0.9674959182739258), ('fixated', 0.9669156670570374), ('sunbed', 0.9666725397109985), ('conquered', 0.9666515588760376), ('Ethelred', 0.9661266207695007)]\n",
            "[('Rendel', 0.9256632924079895), ('Rent', 0.9171224236488342), ('Try', 0.9113222360610962), ('Cory', 0.8927328586578369), ('Cologne', 0.8921950459480286), ('Cole', 0.8877659440040588), ('Pearman', 0.8867085576057434), ('Regent', 0.8842021226882935), ('Marion', 0.8814208507537842), ('Rentokil', 0.8797123432159424)]\n",
            "[('luvvies', 0.9413209557533264), ('junkies', 0.9375405311584473), ('scooters', 0.9302364587783813), ('proxies', 0.9288594126701355), ('patents', 0.9280836582183838), ('printers', 0.9278895854949951), ('puppies', 0.9274654388427734), ('decries', 0.9266148805618286), ('hippies', 0.925669252872467), ('colonies', 0.9251068830490112)]\n",
            "[('proxies', 0.953489363193512), ('luvvies', 0.9511209726333618), ('tions', 0.9478641152381897), ('segments', 0.9416697025299072), ('contortions', 0.9399468898773193), ('tenements', 0.9384333491325378), ('cries', 0.9382924437522888), ('emoluments', 0.9376629590988159), ('decries', 0.9369041323661804), ('junkies', 0.9356029629707336)]\n",
            "[('—that', 0.7336803078651428), ('so—that', 0.7029396891593933), ('it—that', 0.6952650547027588), ('Who', 0.6743254661560059), ('too—that', 0.6720196008682251), ('up—that', 0.6645906567573547), ('no—that', 0.6591740846633911), ('on—that', 0.6580263376235962), ('one—that', 0.6550418734550476), ('As', 0.6520418524742126)]\n",
            "[('Branson', 0.9237784147262573), ('Baldry', 0.9210406541824341), ('Sandy', 0.907724142074585), ('Michel', 0.9073596596717834), ('Belle', 0.9070044755935669), ('Michelle', 0.9069617390632629), ('Boyle', 0.9063393473625183), ('Pearman', 0.9046995639801025), ('Moyle', 0.9000562429428101), ('Carmichael', 0.8957172632217407)]\n",
            "[('£', 0.895292341709137), ('%', 0.864359438419342), ('kW', 0.8192635774612427), ('MW', 0.7995644807815552), ('$', 0.78473961353302), ('mg', 0.7792526483535767), ('km', 0.7385978698730469), ('GW', 0.7256583571434021), ('mph', 0.7169574499130249), ('#', 0.6988896131515503)]\n",
            "[('skate', 0.9068248271942139), ('derogate', 0.9014018774032593), ('confiscate', 0.898630678653717), ('promulgate', 0.8971973657608032), ('inculcate', 0.8964539766311646), ('retaliate', 0.8957490921020508), ('recuperate', 0.8950091600418091), ('reuse', 0.8919953107833862), ('reciprocate', 0.8883776664733887), ('rejuvenate', 0.888034999370575)]\n",
            "[('225', 0.9752001166343689), ('15·4', 0.9707119464874268), ('15.8', 0.9704214334487915), ('5.15', 0.9691325426101685), ('15.6', 0.9685885310173035), ('15·6', 0.9685150980949402), ('15.9', 0.9679380655288696), ('151', 0.9677368998527527), ('230', 0.9677191376686096), ('525', 0.9671908617019653)]\n",
            "[('defamation', 0.9641791582107544), ('saturation', 0.9641662240028381), ('consecration', 0.9626827239990234), ('tenement', 0.9601198434829712), ('proclamation', 0.9593371152877808), ('acclamation', 0.9582403898239136), ('predation', 0.9581596851348877), ('derogation', 0.9570978879928589), ('demarcation', 0.95695960521698), ('conurbation', 0.9557657241821289)]\n",
            "[('Teddington', 0.9452308416366577), ('Cranston', 0.9451489448547363), ('Beddington', 0.9433684349060059), ('Horton', 0.9399299025535583), ('Buxton', 0.9392266869544983), ('Kennington', 0.9387734532356262), ('Addington', 0.9386078715324402), ('Clacton', 0.9379268884658813), ('Bebington', 0.9379158616065979), ('Fenton', 0.9369214773178101)]\n",
            "[('bamboozle', 0.8838752508163452), ('le', 0.870093584060669), ('bubble', 0.8691239356994629), ('shiver', 0.864196240901947), ('polymerase', 0.8639746904373169), ('strawberry', 0.8608870506286621), ('rifle', 0.8591016530990601), ('axle', 0.8579513430595398), ('cradle', 0.8558886051177979), ('staple', 0.8526737689971924)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}