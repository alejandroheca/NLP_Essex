{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R_ZkQp331VX"
      },
      "source": [
        "# Introduction to Word Embeddings\n",
        "\n",
        "## Douglas Rice\n",
        "\n",
        "*This tutorial was originally created by Burt Monroe for his prior work with the Essex Summer School. I've updated and modified it.*\n",
        "\n",
        "In this notebook, we'll estimate our first word embedding model, then go through a series of analyses of the estimated embeddings. After completing this notebook, you should be familar with:\n",
        "\n",
        "\n",
        "1. Preparing a corpus for estimating word embeddings\n",
        "2. Estimating a (static) word embedding model\n",
        "3. Analyzing output of (static) word embedding model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V0jTWdSFY-i"
      },
      "source": [
        "# Data \n",
        "\n",
        "This notebook illustrates the estimation of embeddings on a corpus of Supreme Court oral arguments. The data are available via the excellent Cornell Conversational Analysis Toolkit (ConvoKit). You can read more about the data (and ConvoKit) [here](https://convokit.cornell.edu/documentation/supreme.html#).  The Oral Arguments corpus is described as:\n",
        "\n",
        ">A collection of cases from the U.S. Supreme Court, along with transcripts of oral arguments. Contains approximately 1,700,000 utterances over 8,000 oral arguments transcripts from 7,700 cases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtiK8ZXLR6o2",
        "outputId": "15a1f2fd-219b-4d30-cf00-5e994b086c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting convokit\n",
            "  Downloading convokit-2.5.3.tar.gz (167 kB)\n",
            "     -------------------------------------- 168.0/168.0 kB 1.7 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from convokit) (3.5.1)\n",
            "Requirement already satisfied: pandas>=0.23.4 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from convokit) (1.4.1)\n",
            "Collecting msgpack-numpy>=0.4.3.2\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: spacy>=2.3.5 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from convokit) (3.2.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from convokit) (1.8.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from convokit) (1.0.2)\n",
            "Requirement already satisfied: nltk>=3.4 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from convokit) (3.7)\n",
            "Collecting dill>=0.2.9\n",
            "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
            "     ---------------------------------------- 95.8/95.8 kB 1.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from convokit) (1.1.0)\n",
            "Collecting clean-text>=0.1.1\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting unidecode>=1.1.1\n",
            "  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
            "     -------------------------------------- 235.9/235.9 kB 3.6 MB/s eta 0:00:00\n",
            "Collecting ftfy<7.0,>=6.0\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "     ---------------------------------------- 53.1/53.1 kB 2.9 MB/s eta 0:00:00\n",
            "Collecting emoji<2.0.0,>=1.0.0\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "     -------------------------------------- 175.4/175.4 kB 5.3 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (4.31.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (9.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (1.22.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (1.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.0->convokit) (0.11.0)\n",
            "Collecting msgpack>=0.5.2\n",
            "  Downloading msgpack-1.0.4-cp310-cp310-win_amd64.whl (61 kB)\n",
            "     ---------------------------------------- 61.3/61.3 kB 1.6 MB/s eta 0:00:00\n",
            "Requirement already satisfied: click in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.4->convokit) (8.0.4)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.4->convokit) (2022.3.15)\n",
            "Requirement already satisfied: tqdm in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk>=3.4->convokit) (4.63.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=0.23.4->convokit) (2022.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.20.0->convokit) (3.1.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (2.4.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (8.0.15)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (0.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (2.0.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (62.1.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (1.0.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (3.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (1.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (1.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (0.9.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (3.3.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (0.6.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (2.27.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (3.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from spacy>=2.3.5->convokit) (0.7.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk>=3.4->convokit) (0.4.4)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ftfy<7.0,>=6.0->clean-text>=0.1.1->convokit) (0.2.5)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pathy>=0.3.5->spacy>=2.3.5->convokit) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=2.3.5->convokit) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2021.10.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hermida\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.1)\n",
            "Building wheels for collected packages: convokit, emoji\n",
            "  Building wheel for convokit (setup.py): started\n",
            "  Building wheel for convokit (setup.py): finished with status 'done'\n",
            "  Created wheel for convokit: filename=convokit-2.5.3-py3-none-any.whl size=204129 sha256=0b37d94757100f9b53d24ce67ed4e34597a62e3787eec6f1904dd702ccbe6636\n",
            "  Stored in directory: c:\\users\\hermida\\appdata\\local\\pip\\cache\\wheels\\8f\\da\\dd\\d65869bf6766b536f422e0a9753e9cf98bb9df7904b5b9c4a5\n",
            "  Building wheel for emoji (setup.py): started\n",
            "  Building wheel for emoji (setup.py): finished with status 'done'\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=a79dafa8021e4e3d322022c8678a7e625c50e11eca5ef07697155bcf84cf5bc5\n",
            "  Stored in directory: c:\\users\\hermida\\appdata\\local\\pip\\cache\\wheels\\31\\8a\\8c\\315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built convokit emoji\n",
            "Installing collected packages: msgpack, emoji, unidecode, msgpack-numpy, ftfy, dill, clean-text, convokit\n",
            "  Attempting uninstall: emoji\n",
            "    Found existing installation: emoji 2.0.0\n",
            "    Uninstalling emoji-2.0.0:\n",
            "      Successfully uninstalled emoji-2.0.0\n",
            "Successfully installed clean-text-0.6.0 convokit-2.5.3 dill-0.3.5.1 emoji-1.7.0 ftfy-6.1.1 msgpack-1.0.4 msgpack-numpy-0.4.8 unidecode-1.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip3 install convokit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIDud31zMirS"
      },
      "source": [
        "## Prepping the Corpus\n",
        "\n",
        "The first thing we need to do is to download the corpus. This will take a couple minutes, as this is a large corpus. Lawyers and judges like to talk a lot. The benefit of this additional text, though, is that we have significantly more information for validly estimating the word embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h-2H6qf2SRxL"
      },
      "outputs": [],
      "source": [
        "from convokit import Corpus, download\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWHShYy_S87X",
        "outputId": "7db6c36f-1054-4625-b58e-b472f7a35c43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading supreme-corpus to C:\\Users\\hermida\\.convokit\\downloads\\supreme-corpus\n",
            "Downloading supreme-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/supreme-corpus/supreme-corpus.zip (1255.8MB)... Done\n"
          ]
        }
      ],
      "source": [
        "corpus = Corpus(filename=download(\"supreme-corpus\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mK_mjx1VlZs"
      },
      "source": [
        "We can see a bit of information on our corpus as follows. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qH4C22lTN-e",
        "outputId": "76f0441b-3bc5-4be0-c76c-1114b912fa79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Speakers: 8979\n",
            "Number of Utterances: 1700789\n",
            "Number of Conversations: 7817\n"
          ]
        }
      ],
      "source": [
        "corpus.print_summary_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTBmXmG9Mwcw"
      },
      "source": [
        "Let's look at the first utterance. This is the Chief Justice of the U.S. Supreme Court introducing the case and the first lawyer to speak before the Court."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flNbncFyT5m6",
        "outputId": "7628ef4f-d99b-49b4-ea4b-4d7f35a554ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number 71, Lonnie Affronti versus United States of America.\n",
            "Mr. Murphy.\n"
          ]
        }
      ],
      "source": [
        "for utt in corpus.iter_utterances():\n",
        "    print(utt.text)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-XNgRitMovA"
      },
      "source": [
        "Next, we need to begin to prepare the corpus for estimating word embeddings. To do so, we must first do some standard NLP tasks, segmenting the corpus by sentence and tokenizing the texts. We'll just use the nltk tokenizers to segment into sentences and tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP-wIrM4VKM7",
        "outputId": "28a7bdf2-a240-4e3d-83f6-7d4df001b0ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\hermida\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp_8lT46Mzj1"
      },
      "source": [
        "Let's look at how the tokenizer works for the first utterance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsH8k9vF5LQz",
        "outputId": "5133b503-6ac3-4d26-d2f1-fc1ddc1f961e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Number', '71', ',', 'Lonnie', 'Affronti', 'versus', 'United', 'States', 'of', 'America', '.'], ['Mr.', 'Murphy', '.']]\n"
          ]
        }
      ],
      "source": [
        "for utt in corpus.iter_utterances():\n",
        "    print( [word_tokenize(t) for t in sent_tokenize(utt.text)])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUrQAcZ6U6Bw"
      },
      "source": [
        "Generate the sentence tokens, and the word tokens within them. This took ~ 11 minutes, given 1.7 million utterances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UEEW9D1C9NAo"
      },
      "outputs": [],
      "source": [
        "sents = []\n",
        "for utt in corpus.iter_utterances():\n",
        "    sents.append([word_tokenize(t) for t in sent_tokenize(utt.text)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-THHCld6odo",
        "outputId": "65f9793d-4583-4fe5-c74e-37fc99f8efe9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1700789"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGSiRF8xHzuw",
        "outputId": "3b8e1868-14fd-4b46-c60c-48f77495d89f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['Number',\n",
              "  '71',\n",
              "  ',',\n",
              "  'Lonnie',\n",
              "  'Affronti',\n",
              "  'versus',\n",
              "  'United',\n",
              "  'States',\n",
              "  'of',\n",
              "  'America',\n",
              "  '.'],\n",
              " ['Mr.', 'Murphy', '.']]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxTHaAgPVNho"
      },
      "source": [
        "That's the second document/utterance, a list of lists (each sentence is a list of tokens). That means sents is organized as a list of lists of lists. Word2Vec wants a list of lists (the tokens by sentence, without distinguishing between the utterances in which they are used). So, we flatten the list (to a list of sentences, each a list of tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HqZrugUSN5cD"
      },
      "outputs": [],
      "source": [
        "flat_sents_list = [sentence for utt in sents for sentence in utt] # for every utterance, loop over its sentences and add them to the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Number',\n",
              " '71',\n",
              " ',',\n",
              " 'Lonnie',\n",
              " 'Affronti',\n",
              " 'versus',\n",
              " 'United',\n",
              " 'States',\n",
              " 'of',\n",
              " 'America',\n",
              " '.']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flat_sents_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fi0uuGzW4fh",
        "outputId": "8a901f75-ddcb-43a2-aa33-56bda7b1b173"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3880254"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(flat_sents_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxu2thVkqslh"
      },
      "source": [
        "As you can see, we are closing in on 4 million sentences overall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X55IdhZbNObm"
      },
      "source": [
        "## Estimate word2vec Embeddings\n",
        "\n",
        "First, we'll estimate word2vec embeddings. We'll use gensim, a well-known (and fast!) library for estimating embeddings and topic models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6Tvv8w9IHiX"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwP_CUH_V77e"
      },
      "source": [
        "To estimate the word2vec model, we need to specify a number of parameters. The first argument we pass is the sentence object that we've just created (`flat_sents_list`). \n",
        "\n",
        "From there, we need to specify the dimensionality (or size) of the estimated vectors. I used the default dimensionality of 100. I set the context window at 5; we will play around with estimates based on other windows below. Note that we also have retained all tokens to this point. In estimating the model, we can specify the terms to retain though (which can enhance our computational speed); the min_count of token frequency defaults to 1, but I set it at 5, which will still probably be a bit noisy. According to the gensim docs, a random seed is always set to 1, but to ensure replicability, you need to use only one worker/thread, which I think is all Google Colab will give anyway.\n",
        "\n",
        "You'll have to wait a bit again. This took approximately 11 minutes on a recent run. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SntK0q0KfHm"
      },
      "outputs": [],
      "source": [
        "model_w5 = Word2Vec(sentences=flat_sents_list, size=100, window=5, min_count=5, workers=1)\n",
        "model_w5.save(\"w5_word2vec.model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFaqh8DuNWWi"
      },
      "source": [
        "Now let's see what words are near each other; given the setting, let's start with a term likely to pop up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvXzrgtdPNkG",
        "outputId": "467af952-b6d6-4df6-b1e1-61abe88f2cfc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('policeman', 0.686346709728241),\n",
              " ('arresting', 0.6797668933868408),\n",
              " ('detective', 0.6674090623855591),\n",
              " ('commanding', 0.6645101308822632),\n",
              " ('Kallnischkies', 0.6578880548477173),\n",
              " ('sheriff', 0.6559174656867981),\n",
              " ('FBI', 0.635968804359436),\n",
              " ('plainclothes', 0.6321883201599121),\n",
              " ('officers', 0.6278793811798096),\n",
              " ('corrections', 0.6276471614837646)]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_w5.wv.most_similar(\"police\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgCkfw5HNlpt"
      },
      "source": [
        "All of the most similar words are one that we would expect to see in this context. One thing you might notice here is that we haven't done anything with capitalization; see how \"FBI\" and \"Border\" are capitalized? That means we are capturing subtle (and not-so-subtle) differences in what words mean. So what happens if we capitalize \"Police\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hArVKq6wQAD1",
        "outputId": "b6bd625d-acff-4246-9dfc-e0abb926a522"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Fire', 0.8665454387664795),\n",
              " ('Ships', 0.8045356273651123),\n",
              " ('Detectives', 0.783249020576477),\n",
              " ('Correction', 0.7493676543235779),\n",
              " ('Commander', 0.7446268796920776),\n",
              " ('Corrections', 0.7420327067375183),\n",
              " ('Bookkeeping', 0.7297062873840332),\n",
              " ('Herder', 0.7293038368225098),\n",
              " ('Engineer', 0.7246467471122742),\n",
              " ('Staff', 0.7201530337333679)]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_w5.wv.most_similar(\"Police\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UoIKCQOdHol"
      },
      "source": [
        "Much different! This is catching some related professions (\"Detectives\", \"Corrections\"), but also just seems to be capturing professions generally. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54KJAKL0OD5Q"
      },
      "source": [
        "As we discussed in class, the classic analogy is (Man is to woman, as king is to ____.) Of course, at oral argument for the Supreme Court, we are exceedingly unlikely to see the terms \"king\" and \"queen\" used very often, which might limit the utility of the analogy. Here's a quick check. The basic idea is to calculate the vector from `woman` + `king` - `man`, then look for the most similar vectors. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-GnHgW-QLwp",
        "outputId": "9999586f-f038-4cf6-93cc-0f8982f05a2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('German', 0.5547077655792236),\n",
              " ('Yugoslavia', 0.5521583557128906),\n",
              " ('mother', 0.5404218435287476),\n",
              " ('Mexican', 0.5387974977493286),\n",
              " ('native', 0.5348100662231445),\n",
              " ('husband', 0.5322563648223877),\n",
              " ('marrying', 0.5303377509117126),\n",
              " ('descent', 0.5277263522148132),\n",
              " ('YMCA', 0.5270720720291138),\n",
              " ('Netherlands', 0.5259039402008057)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_w5.wv.most_similar(positive=[\"woman\",\"king\"],negative=[\"man\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTx5JsvfdrI8"
      },
      "source": [
        "Yeah, that's not making much sense. Let's look at the most similar terms for \"king\" to see how it worked. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiuPfYQgdzmd",
        "outputId": "20f822e2-1019-424b-a206-d1635bdcd49c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('crowd', 0.5560708045959473),\n",
              " ('parliament', 0.5532700419425964),\n",
              " ('crown', 0.5440690517425537),\n",
              " ('YMCA', 0.5377370715141296),\n",
              " ('Netherlands', 0.5282279253005981),\n",
              " ('helicopter', 0.5278602838516235),\n",
              " ('bartender', 0.5258022546768188),\n",
              " ('village', 0.517535388469696),\n",
              " ('disappearing', 0.515839695930481),\n",
              " ('Canadians', 0.5113198757171631)]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_w5.wv.most_similar(\"king\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvXWRuybd8bh"
      },
      "source": [
        "# A Tangent on Bias\n",
        "\n",
        "As we discussed in class, word embeddings have proven to be a useful tool for uncovering/revealing bias in large corpora. Here, we can see how well the U.S. Supreme Court fares. We'll look at occupations. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EHjaWj1Q6ie",
        "outputId": "78c6300b-5f63-4d75-c978-5a5b3f670709"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('extraction', 0.6386083364486694),\n",
              " ('offspring', 0.6179507374763489),\n",
              " ('employment', 0.6173360347747803),\n",
              " ('impairment', 0.6090482473373413),\n",
              " ('infant', 0.5856988430023193),\n",
              " ('engagement', 0.5855349898338318),\n",
              " ('marriage', 0.5724813342094421),\n",
              " ('nationality', 0.5723633766174316),\n",
              " ('unborn', 0.5705928802490234),\n",
              " ('placement', 0.5701239109039307)]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_w5.wv.most_similar(positive=[\"woman\",\"occupation\"],negative=['man'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AT6TXy8sRDvL",
        "outputId": "89b78f3b-2e62-435f-a51e-2a293ff4488d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('engagement', 0.6586723327636719),\n",
              " ('installation', 0.6567591428756714),\n",
              " ('occupancy', 0.6194782853126526),\n",
              " ('enterprise', 0.6189873218536377),\n",
              " ('outlet', 0.6179249286651611),\n",
              " ('activity', 0.6159071922302246),\n",
              " ('operator', 0.6034233570098877),\n",
              " ('aircraft', 0.5891773104667664),\n",
              " ('agent', 0.5782469511032104),\n",
              " ('operation', 0.574250340461731)]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_w5.wv.most_similar(positive=[\"man\",\"occupation\"],negative=[\"woman\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGaGPvmRAqSe"
      },
      "source": [
        "You can see from the above that women's employment is routinely discussed with respect to marriage and reproduction, a dynamic totally absent in the `man` example. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsKGecGLCP9X"
      },
      "source": [
        "# Working with Estimated Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCRe9_uGDfNq"
      },
      "source": [
        "Once we've estimated the embeddings, there are a number of other options for analysis beyond the simple vector operations above. In this section, we'll look at how the estimated vectors cluster together into coherent themes. To do so, we load `numpy` (for extracting the vectors as arrays) and the KMeans library (for estimating the clustering algorithm). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLqDhhELSu7v"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhhoh29td5LZ"
      },
      "outputs": [],
      "source": [
        "wv_w5 = model_w5.wv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lNXQUO6hPa2"
      },
      "outputs": [],
      "source": [
        "# extract the words & their vectors, as numpy arrays\n",
        "vectors_w5 = np.asarray(model_w5.wv.vectors)\n",
        "labels_w5 = np.asarray(model_w5.wv.index2word)  # fixed-width numpy strings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzReKyv0D6s5"
      },
      "source": [
        "We can check the dimension of the embedding vectors that we've extracted. Note that they are equal to the number of words by the number of dimensions; so we have a weighted distribution over 100 dimensions for 61,103 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAB-cDsGiIRl",
        "outputId": "df7e2905-96c0-4e6d-d3a8-2909511ca59d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(61103, 100)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectors_w5.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUYboiXzEND0"
      },
      "source": [
        "With that in hand, we can estimate a simple clustering algorithm. We specify 20 clusters, but feel free to play around with that number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZotbWIIkAx3",
        "outputId": "be997e2b-26ae-4ffd-b3f4-84fea2e1143d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KMeans(n_clusters=20)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kmeans_w5_20 = KMeans(n_clusters=20)\n",
        "kmeans_w5_20.fit(vectors_w5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3-8jBkbkmqO",
        "outputId": "95877450-5195-4c3f-b99b-95813794b340"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(61103,)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kmeans_w5_20.labels_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNz43RmGn3Au",
        "outputId": "6dadec24-f02f-418a-f38c-3761f932cce2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(20, 100)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kmeans_w5_20.cluster_centers_.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOii-jiPEeAB"
      },
      "source": [
        "Note what we have estimated with KMeans. We have 20 cluster centers, each of 100 dimensions, the same number of dimensions that we have for each of our tokens. Therefore, we look for which of the tokens are most similar to one of our cluster centers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hux8G39TnWqQ",
        "outputId": "9e890d7d-ddd4-4c7b-b1e3-015513e29da6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('attacked', 0.817511260509491),\n",
              " ('sanctioned', 0.7676904797554016),\n",
              " ('harmed', 0.76265549659729),\n",
              " ('condemned', 0.7622823119163513),\n",
              " ('misled', 0.7566522359848022),\n",
              " ('replaced', 0.7562367916107178),\n",
              " ('tested', 0.7554956674575806),\n",
              " ('criticized', 0.753906786441803),\n",
              " ('victimized', 0.7522069215774536),\n",
              " ('pursued', 0.7501586079597473)]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_w5.wv.most_similar([kmeans_w5_20.cluster_centers_[1]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIc6Kx1sOVUf"
      },
      "source": [
        "Now let's loop over each of the cluster centers for the most similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG1Ik55Mo-29",
        "outputId": "76adce8f-15a3-4637-e0ca-610438433409"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Orville', 0.8814734220504761), ('Kresge', 0.8752458095550537), ('Frana', 0.8727689385414124), ('Followed', 0.8725795745849609), ('Ivanovich', 0.8725254535675049), ('Wash', 0.8718684911727905), ('Deegan', 0.8708628416061401), ('G.R.H', 0.8643906116485596), ('Y.', 0.8628705143928528), ('Shops', 0.8625250458717346)]\n",
            "[('attacked', 0.817511260509491), ('sanctioned', 0.7676904797554016), ('harmed', 0.76265549659729), ('condemned', 0.7622823119163513), ('misled', 0.7566522359848022), ('replaced', 0.7562367916107178), ('tested', 0.7554956674575806), ('criticized', 0.753906786441803), ('victimized', 0.7522069215774536), ('pursued', 0.7501586079597473)]\n",
            "[('130', 0.9330103397369385), ('45', 0.9282690286636353), ('25', 0.9267070293426514), ('75', 0.9163283109664917), ('240', 0.911709189414978), ('140', 0.9107958078384399), ('17', 0.9082223176956177), ('70', 0.9067625999450684), ('55', 0.902971625328064), ('15', 0.9015152454376221)]\n",
            "[('1968', 0.9276836514472961), ('1956', 0.9260790348052979), ('1963', 0.922167181968689), ('1990', 0.9172959923744202), ('1959', 0.9154037237167358), ('1976', 0.9148420095443726), ('1951', 0.9137532114982605), ('1965', 0.9136273860931396), ('1953', 0.912916362285614), ('1980', 0.9117632508277893)]\n",
            "[('dismissal', 0.8050606846809387), ('reopening', 0.787096381187439), ('reclassification', 0.7520871162414551), ('reexamination', 0.7319624423980713), ('removal', 0.7302449941635132), ('suspension', 0.7249616980552673), ('intervention', 0.7204098701477051), ('deportation', 0.714582085609436), ('denaturalization', 0.7130538821220398), ('vacatur', 0.7107030749320984)]\n",
            "[('models', 0.7835787534713745), ('divisions', 0.7724484205245972), ('illustrations', 0.7709071636199951), ('licensees', 0.769799530506134), ('topics', 0.768871545791626), ('variables', 0.7653291821479797), ('industries', 0.7628283500671387), ('possibilities', 0.7628145217895508), ('ingredients', 0.7620559930801392), ('entities', 0.7613664865493774)]\n",
            "[('Building', 0.8931530714035034), ('Community', 0.8682875037193298), ('Supply', 0.8680365085601807), ('Trades', 0.8651673793792725), ('Coal', 0.8634456396102905), ('Research', 0.8550739288330078), ('Educational', 0.8547570109367371), ('Liquor', 0.8524138927459717), ('Producers', 0.8515104055404663), ('Bridge', 0.8473158478736877)]\n",
            "[('factory', 0.8767657279968262), ('lumber', 0.858423113822937), ('trailer', 0.8559240102767944), ('grain', 0.8442138433456421), ('metal', 0.843806266784668), ('tank', 0.8431904315948486), ('warehouse', 0.8418574929237366), ('shops', 0.8349717259407043), ('meat', 0.8329246640205383), ('locomotive', 0.826590895652771)]\n",
            "[('gives', 0.7806172370910645), ('puts', 0.7765729427337646), ('allows', 0.7684574723243713), ('identifies', 0.762449324131012), ('accepts', 0.756497859954834), ('considers', 0.7549189925193787), ('denies', 0.737999439239502), ('invokes', 0.7370607256889343), ('leaves', 0.7350805401802063), ('requires', 0.7339970469474792)]\n",
            "[('Henderson', 0.9481356739997864), ('Robertson', 0.9434297680854797), ('Brooks', 0.9425191283226013), ('Collins', 0.9412640333175659), ('Palmer', 0.9383159279823303), ('Reid', 0.9378915429115295), ('Woods', 0.9359888434410095), ('Ullman', 0.9350318312644958), ('Neil', 0.9345647096633911), ('Matthews', 0.933966875076294)]\n",
            "[('confine', 0.8125970363616943), ('abandon', 0.8022773265838623), ('observe', 0.7993257641792297), ('accommodate', 0.790529727935791), ('resist', 0.7896234393119812), ('utilize', 0.7889550924301147), ('assign', 0.7867074608802795), ('pursue', 0.7845375537872314), ('ignore', 0.7828922271728516), ('hold', 0.7780699729919434)]\n",
            "[('problematic', 0.8496720194816589), ('dubious', 0.8248215913772583), ('persuasive', 0.8184930086135864), ('bizarre', 0.8052441477775574), ('apt', 0.7995887994766235), ('attenuated', 0.7993295192718506), ('confusing', 0.7879084348678589), ('remarkable', 0.7843382954597473), ('shocking', 0.7826062440872192), ('risky', 0.776463508605957)]\n",
            "[('McGrath', 0.9435992240905762), ('Ackerman', 0.9389046430587769), (\"O'Donnell\", 0.9351419806480408), ('Donahue', 0.9344121217727661), ('Devine', 0.9309052228927612), ('Hagan', 0.9304224252700806), ('Carlisle', 0.9283851385116577), ('McClain', 0.9265328049659729), ('Locke', 0.9250638484954834), ('Ludwig', 0.9249386191368103)]\n",
            "[('promoting', 0.7277693748474121), ('eliminating', 0.6923742890357971), ('advancement', 0.6819792985916138), ('external', 0.680986225605011), ('anti-competitive', 0.6671347618103027), ('facilitating', 0.6619493365287781), ('pro-competitive', 0.655624508857727), ('expressive', 0.651620626449585), ('influencing', 0.6479772329330444), ('impeding', 0.6472119092941284)]\n",
            "[('awareness', 0.7109338045120239), ('notion', 0.7008355855941772), ('criticism', 0.685747504234314), ('concept', 0.6801192760467529), ('attitude', 0.668993353843689), ('idea', 0.6647405624389648), ('conception', 0.6576854586601257), ('concern', 0.6533582210540771), ('philosophy', 0.6499685049057007), ('analysis', 0.6444532871246338)]\n",
            "[('shops', 0.8112660646438599), ('sheep', 0.8009685277938843), ('bread', 0.7945319414138794), ('towers', 0.788564920425415), ('apartments', 0.7801002264022827), ('factories', 0.778394341468811), ('horses', 0.7761141061782837), ('patrons', 0.7747900485992432), ('factory', 0.7599202990531921), ('renting', 0.7550193071365356)]\n",
            "[('doctor', 0.8299083113670349), ('man', 0.772124707698822), ('soldier', 0.771426260471344), ('person', 0.770720362663269), ('lawyer', 0.7537769079208374), ('sheriff', 0.7515032291412354), ('he', 0.7481102347373962), ('patient', 0.7469408512115479), ('defendant', 0.7436161041259766), ('supervisor', 0.7429200410842896)]\n",
            "[('premiums', 0.8231333494186401), ('premium', 0.8215950727462769), ('indebtedness', 0.8080024719238281), ('rents', 0.8073433637619019), ('dividends', 0.8031982183456421), ('royalties', 0.7973870038986206), ('rentals', 0.7877181768417358), ('payment', 0.7830938100814819), ('cash', 0.7714519500732422), ('profits', 0.7713093757629395)]\n",
            "[('Omnibus', 0.8454455733299255), ('Landrum-Griffin', 0.8329422473907471), ('Prohibition', 0.8251340389251709), ('Bases', 0.8165175914764404), ('Urgent', 0.810247540473938), ('Corrupt', 0.8075742721557617), ('Lloyd-La', 0.8049132823944092), ('Wage', 0.804621696472168), ('Partnership', 0.8023871183395386), ('Tariff', 0.802263617515564)]\n",
            "[('Finding', 0.8717832565307617), ('Footnotes', 0.8614867925643921), ('Paragraph', 0.8612054586410522), ('Note', 0.8592162728309631), ('Exhibits', 0.847719669342041), ('CFR', 0.8240364193916321), ('Numbers', 0.8234137296676636), ('Pages', 0.8191646933555603), ('C.F.R', 0.8177067041397095), ('Stat', 0.8125406503677368)]\n"
          ]
        }
      ],
      "source": [
        "for k in range(20):\n",
        "  print(model_w5.wv.most_similar([kmeans_w5_20.cluster_centers_[k]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhXDgcZxOaDy"
      },
      "source": [
        "# Window Size\n",
        "\n",
        "What's the effect of choices over window size? Let's play around and find out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZNkNfUvlGxU"
      },
      "outputs": [],
      "source": [
        "model_w1 = Word2Vec(sentences=flat_sents_list, size=100, window=1, min_count=5, workers=1)\n",
        "model_w1.save(\"w1_word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCUGIEUOrJY1",
        "outputId": "1e086742-4007-48a0-dba1-3fbac9a7f39d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KMeans(n_clusters=20)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectors_w1 = np.asarray(model_w1.wv.vectors)\n",
        "labels_w1 = np.asarray(model_w1.wv.index2word)\n",
        "\n",
        "kmeans_w1_20 = KMeans(n_clusters=20)\n",
        "kmeans_w1_20.fit(vectors_w1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH_pUJ-arulf",
        "outputId": "8ec34ffd-f2ff-4b4a-b673-9c8adff34534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('185', 0.9471156001091003), ('272', 0.9401297569274902), ('255', 0.9379180073738098), ('227', 0.9356634020805359), ('186', 0.9354671239852905), ('352', 0.9337529540061951), ('167', 0.9334511160850525), ('398', 0.9321123957633972), ('199', 0.9266774654388428), ('683', 0.9257817268371582)]\n",
            "[('supervisor', 0.7971924543380737), ('laborer', 0.7777252197265625), ('doctor', 0.7767788171768188), ('parent', 0.7759108543395996), ('customer', 0.7593592405319214), ('nonmember', 0.7578420639038086), ('student', 0.7573502063751221), ('nonprofessional', 0.7480571269989014), ('dissident', 0.7470308542251587), ('banker', 0.7456125020980835)]\n",
            "[('deceived', 0.828521728515625), ('captured', 0.826597273349762), ('attacked', 0.8232210874557495), ('terminated', 0.8220384120941162), ('disclosed', 0.821386992931366), ('represented', 0.8101081252098083), ('revealed', 0.8093607425689697), ('admitted', 0.8074983358383179), ('evaluated', 0.8072065114974976), ('replaced', 0.8067431449890137)]\n",
            "[('2004', 0.894740879535675), ('1984', 0.8844207525253296), ('2008', 0.8827054500579834), (\"'74\", 0.8822766542434692), ('1987', 0.8819397687911987), ('1968', 0.8774742484092712), (\"'78\", 0.8766812682151794), ('2003', 0.876228928565979), ('1946', 0.8760514259338379), (\"'75\", 0.8738600015640259)]\n",
            "[('application', 0.7469912171363831), ('implementation', 0.740951657295227), ('condition', 0.7367810606956482), ('recognition', 0.7359259128570557), ('consideration', 0.7345470786094666), ('creation', 0.7297468781471252), ('determination', 0.7288891077041626), ('exercise', 0.7284927368164062), ('elimination', 0.7280939817428589), ('performance', 0.7223453521728516)]\n",
            "[('Killian', 0.9625125527381897), ('Ponder', 0.9622888565063477), ('Farley', 0.9604581594467163), ('Loeb', 0.9602168798446655), ('Gladden', 0.9590149521827698), ('Cahn', 0.9569314122200012), ('Acker', 0.9565562009811401), ('Phelan', 0.9562298655509949), ('Zagel', 0.9557970762252808), ('Durkin', 0.9555537104606628)]\n",
            "[('ameliorative', 0.7792061567306519), ('liquidity', 0.7706766128540039), ('ecological', 0.7648706436157227), ('repression', 0.7634700536727905), ('looseness', 0.7602947354316711), ('prevalence', 0.7536088228225708), ('deconcentration', 0.7485971450805664), ('ethic', 0.748355507850647), ('bridging', 0.739762544631958), ('specialization', 0.7369058728218079)]\n",
            "[('locals', 0.810451865196228), ('borrowers', 0.795788586139679), ('households', 0.7895301580429077), ('combinations', 0.7793295383453369), ('businesses', 0.7792305946350098), ('designs', 0.7784757614135742), ('establishments', 0.7764928340911865), ('brands', 0.7710673809051514), ('topics', 0.7696681022644043), ('instruments', 0.768075704574585)]\n",
            "[('saying', 0.7535449862480164), ('denies', 0.7445228099822998), ('deprives', 0.7416976094245911), ('says', 0.7385255098342896), ('treats', 0.7375626564025879), ('removes', 0.7374830842018127), ('binds', 0.7373650074005127), ('invoking', 0.7366828322410583), ('defends', 0.7358736991882324), ('accepts', 0.735292375087738)]\n",
            "[('cigar', 0.8634065985679626), ('trailer', 0.8406097292900085), ('jet', 0.838897705078125), ('lumber', 0.8376830816268921), ('bowling', 0.8336997032165527), ('metal', 0.827592134475708), ('septic', 0.8272629976272583), ('soybean', 0.8234801292419434), ('inbound', 0.8223119378089905), ('bagged', 0.8203325867652893)]\n",
            "[('dubious', 0.8242177963256836), ('risky', 0.7995220422744751), ('shocking', 0.798342227935791), ('innocuous', 0.7967112064361572), ('nebulous', 0.795096755027771), ('benign', 0.7908626198768616), ('suggestive', 0.7883968353271484), ('pernicious', 0.7842426300048828), ('bizarre', 0.7829235196113586), ('attenuated', 0.7782316207885742)]\n",
            "[('Nathan', 0.9362592697143555), ('Jerome', 0.931183397769928), ('Paine', 0.9275413751602173), ('Stuart', 0.9207258820533752), ('Pound', 0.9206097722053528), ('Marr', 0.9203298687934875), ('Jennings', 0.918940007686615), ('Keith', 0.9188413023948669), ('Patricia', 0.9178427457809448), ('Amos', 0.9166817665100098)]\n",
            "[('expose', 0.8594632148742676), ('treat', 0.8535485863685608), ('accommodate', 0.8529320955276489), ('pursue', 0.8525835275650024), ('assign', 0.8504568338394165), ('invoke', 0.8465836048126221), ('remove', 0.8436182737350464), ('observe', 0.8421039581298828), ('condemn', 0.8398715853691101), ('restore', 0.8390454053878784)]\n",
            "[('Waterways', 0.9380391836166382), ('Terminals', 0.9295241832733154), ('Distributors', 0.920191764831543), ('Bus', 0.9172830581665039), ('Taxicab', 0.9164425134658813), ('Bend', 0.9109480977058411), ('Producers', 0.910570502281189), ('Calcasieu', 0.905274510383606), ('Cooks', 0.9051232933998108), ('View', 0.9008263945579529)]\n",
            "[('actually', 0.7886269092559814), ('properly', 0.7759848237037659), ('effectively', 0.7626427412033081), ('always', 0.7587287425994873), ('reasonably', 0.7486608028411865), ('validly', 0.7383886575698853), ('fully', 0.729160726070404), ('definitely', 0.7287191152572632), ('otherwise', 0.7283117175102234), ('legally', 0.7256502509117126)]\n",
            "[('Marketing', 0.9590170383453369), ('Inspection', 0.9221042394638062), ('Enforcement', 0.9139934778213501), ('Protective', 0.9128321409225464), ('Banking', 0.9126186966896057), ('Resource', 0.911016583442688), ('Disposal', 0.9105144739151001), ('Practice', 0.9062747955322266), ('Conservation', 0.904113233089447), ('Refuge', 0.9035392999649048)]\n",
            "[('promotional', 0.7720459699630737), ('retention', 0.7669454216957092), ('rehabilitative', 0.7426027059555054), ('pooling', 0.7416107058525085), ('promotion', 0.7393273115158081), ('long-term', 0.7370563745498657), ('relocation', 0.7359353303909302), ('developmental', 0.7316608428955078), ('preexisting', 0.7308710813522339), ('licensing', 0.7293417453765869)]\n",
            "[('Aren', 0.9653787612915039), ('Couldn', 0.9602600932121277), ('Wasn', 0.9577127695083618), ('Hasn', 0.9532082080841064), ('Wouldn', 0.9505094289779663), ('Wahpeton', 0.9415474534034729), ('Refresh', 0.9392900466918945), ('G-I-Z', 0.938677191734314), ('Shouldn', 0.9361728429794312), ('71a', 0.9360338449478149)]\n",
            "[('wanted', 0.8737202286720276), ('wants', 0.8303227424621582), ('can', 0.8162819147109985), ('want', 0.8148002624511719), ('will', 0.8103973865509033), ('intended', 0.805923342704773), ('able', 0.8034945726394653), ('willing', 0.8018330335617065), ('obliged', 0.797982931137085), ('supposed', 0.7978895902633667)]\n",
            "[('cones', 0.8447377681732178), ('canoes', 0.8400932550430298), ('glycerin', 0.8331191539764404), ('resells', 0.8325679302215576), ('tyrants', 0.8271706104278564), ('manipulates', 0.8166558742523193), ('lunchroom', 0.8154339790344238), ('hosiery', 0.8149846792221069), ('hamburgers', 0.8141247034072876), ('mules', 0.8139330148696899)]\n"
          ]
        }
      ],
      "source": [
        "for k in range(20):\n",
        "  print(model_w1.wv.most_similar([kmeans_w1_20.cluster_centers_[k]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmaO5ktNOc-E"
      },
      "source": [
        "Window of 30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynYxtIfjsINr"
      },
      "outputs": [],
      "source": [
        "model_w30 = Word2Vec(sentences=flat_sents_list, size=100, window=30, min_count=2, workers=4)\n",
        "model_w30.save(\"w30_word2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnDBe75IucOQ",
        "outputId": "7b70cda1-ef13-453f-abd7-ca45407364c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KMeans(n_clusters=20)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectors_w30 = np.asarray(model_w30.wv.vectors)\n",
        "labels_w30 = np.asarray(model_w30.wv.index2word)\n",
        "\n",
        "kmeans_w30_20 = KMeans(n_clusters=20)\n",
        "kmeans_w30_20.fit(vectors_w30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSmjmtjHvJUP",
        "outputId": "6da3a60c-ec37-48e7-9509-6ef52ed7ea72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Cannel', 0.8970688581466675), ('Bud', 0.8745160102844238), ('Et', 0.8718833923339844), ('Larrimer', 0.8678923845291138), ('Jochnowitz', 0.8659942150115967), ('Schmechel', 0.8649727702140808), ('Eggers', 0.8629851341247559), ('Rauch', 0.8628865480422974), ('Sperling', 0.8627452850341797), ('Lockton', 0.8580783605575562)]\n",
            "[('kicked', 0.8909618258476257), ('threw', 0.8817209005355835), ('thrown', 0.8754757642745972), ('backed', 0.857831597328186), ('knocked', 0.8560508489608765), ('walked', 0.8446695804595947), ('throw', 0.842941164970398), ('turned', 0.8425027132034302), ('stepped', 0.8380867838859558), ('pull', 0.8378132581710815)]\n",
            "[('Blackmun', 0.9822768568992615), ('Kennedy', 0.9806851744651794), ('Brennan', 0.9784543514251709), ('Gorsuch', 0.978076696395874), ('Sotomayor', 0.9751110076904297), ('Kagan', 0.9750866293907166), ('Alito', 0.9750633835792542), ('Harlan', 0.9749494791030884), ('Kavanaugh', 0.9743109941482544), ('Rehnquist', 0.9738765954971313)]\n",
            "[('mechanisms', 0.7557004690170288), ('regimes', 0.7509713172912598), ('illustrations', 0.7455032467842102), ('versions', 0.7407181262969971), ('passages', 0.7383244037628174), ('topics', 0.7264369130134583), ('forms', 0.7225063443183899), ('rationales', 0.7196061015129089), ('groups', 0.7171143293380737), ('features', 0.7163488268852234)]\n",
            "[('confine', 0.8173251748085022), ('accommodate', 0.7883321046829224), ('invoke', 0.7737467885017395), ('anticipate', 0.7646559476852417), ('undertake', 0.7616567611694336), ('argue', 0.7576863169670105), ('observe', 0.752429723739624), ('abandon', 0.7494666576385498), ('hold', 0.7491470575332642), ('attach', 0.7475476264953613)]\n",
            "[('Paragraph', 0.815934419631958), ('Exhibits', 0.8023421764373779), ('Footnotes', 0.7921990156173706), ('Pages', 0.7906903028488159), ('pages', 0.7856848239898682), ('cents', 0.7843772172927856), ('C.F.R', 0.7804523706436157), ('Paragraphs', 0.7779064178466797), ('Footnote', 0.7768104076385498), ('USC', 0.772202730178833)]\n",
            "[('attacked', 0.7411386966705322), ('invoked', 0.7148436307907104), ('represented', 0.7002719044685364), ('challenged', 0.6977235674858093), ('tested', 0.697300910949707), ('advised', 0.6960781812667847), ('utilized', 0.6939775347709656), ('persuaded', 0.69388747215271), ('determined', 0.691202700138092), ('identified', 0.690778911113739)]\n",
            "[('petroleum', 0.8499523997306824), ('suppliers', 0.8152587413787842), ('pipelines', 0.8077085018157959), ('generator', 0.8061815500259399), ('gasoline', 0.8024270534515381), ('commodity', 0.7964016199111938), ('products', 0.7950727939605713), ('copper', 0.7891091704368591), ('manufacture', 0.7864784598350525), ('manufacturers', 0.7860286235809326)]\n",
            "[('Collins', 0.8859153985977173), ('Gordon', 0.8767116665840149), ('Robertson', 0.8718130588531494), ('Reid', 0.8653037548065186), ('Coleman', 0.8640297651290894), ('Cole', 0.8625041246414185), ('Brooks', 0.859857439994812), ('Jordan', 0.858433723449707), ('Moore', 0.8566857576370239), ('Martin', 0.8555644750595093)]\n",
            "[('25', 0.9276760220527649), ('17', 0.9073329567909241), ('35', 0.8979383707046509), ('85', 0.8948237895965576), ('13', 0.8930455446243286), ('10', 0.8898394107818604), ('15', 0.8883296847343445), ('27', 0.8851974606513977), ('75', 0.8821730613708496), ('20', 0.8819403052330017)]\n",
            "[('sheep', 0.7638990879058838), ('factories', 0.7607324123382568), ('shops', 0.7585659623146057), ('Chevrolet', 0.7407190799713135), ('residences', 0.7400531768798828), ('metal', 0.7396716475486755), ('factory', 0.7374400496482849), ('band', 0.7360804080963135), ('cooking', 0.733991265296936), ('farms', 0.7303016185760498)]\n",
            "[('crime', 0.7748416066169739), ('homicide', 0.7574265003204346), ('accused', 0.7561053037643433), ('murderer', 0.744107723236084), ('slaying', 0.7392375469207764), ('killing', 0.7329660058021545), ('intoxication', 0.7327494025230408), ('victim', 0.7276337146759033), ('defendant', 0.6997662782669067), ('murder', 0.6936355829238892)]\n",
            "[('Community', 0.8125040531158447), ('Works', 0.7905410528182983), ('Supply', 0.7890875339508057), ('Center', 0.7725169062614441), ('Development', 0.7675082683563232), ('Freight', 0.7653668522834778), ('Research', 0.7574746608734131), ('Urban', 0.7502559423446655), ('Storage', 0.7475212812423706), ('Delta', 0.7455506324768066)]\n",
            "[('yard', 0.8585683703422546), ('garage', 0.8481907844543457), ('roof', 0.8368858098983765), ('plane', 0.8337648510932922), ('driveway', 0.8328049182891846), ('boat', 0.8233925104141235), ('lights', 0.8232897520065308), ('tank', 0.8227328062057495), ('lawn', 0.820040762424469), ('grass', 0.8197389841079712)]\n",
            "[('dismissal', 0.7931605577468872), ('appeal', 0.7617639303207397), ('intervention', 0.7310522794723511), ('reconsideration', 0.7071701288223267), ('reopening', 0.6910331845283508), ('merits', 0.6882840394973755), ('appealability', 0.6724062561988831), ('1292', 0.6682615876197815), ('motion', 0.6682438850402832), ('reexamination', 0.6675931215286255)]\n",
            "[('payment', 0.8312336802482605), ('premiums', 0.8080623149871826), ('debt', 0.7794533967971802), ('indebtedness', 0.7781884074211121), ('payments', 0.7705186605453491), ('policyholders', 0.7691868543624878), ('debts', 0.7664485573768616), ('premium', 0.7646856307983398), ('moneys', 0.763879656791687), ('earnings', 0.7586170434951782)]\n",
            "[('1926', 0.8685393333435059), ('1936', 0.8349576592445374), ('1976', 0.8255058526992798), ('1940', 0.8252383470535278), ('1949', 0.8223583698272705), ('1917', 0.8207710981369019), ('1918', 0.818161129951477), ('1986', 0.8151358962059021), ('1944', 0.8147996664047241), ('1948', 0.813517689704895)]\n",
            "[('destructive', 0.6725175976753235), ('amorphous', 0.67240309715271), ('pervasive', 0.6679911613464355), ('vital', 0.6595221757888794), ('significant', 0.6526758670806885), ('subtle', 0.6407512426376343), ('flexible', 0.6336971521377563), ('pragmatic', 0.6328463554382324), ('dubious', 0.6311514377593994), ('sensitive', 0.6297839879989624)]\n",
            "[('accepts', 0.8115676641464233), ('allows', 0.8057781457901001), ('gives', 0.8031413555145264), ('requires', 0.7591423988342285), ('identifies', 0.75813889503479), ('considers', 0.7568320035934448), ('puts', 0.7556962966918945), ('asserts', 0.7490133047103882), ('denies', 0.7436589002609253), ('leaves', 0.742215633392334)]\n",
            "[('recognition', 0.6727844476699829), ('concept', 0.664700448513031), ('notion', 0.6604673266410828), ('expression', 0.6393445730209351), ('philosophy', 0.6391103863716125), ('idea', 0.6365021467208862), ('principle', 0.623900294303894), ('attitude', 0.6226199269294739), ('conception', 0.5950179100036621), ('goal', 0.5946863293647766)]\n"
          ]
        }
      ],
      "source": [
        "for k in range(20):\n",
        "  print(model_w30.wv.most_similar([kmeans_w30_20.cluster_centers_[k]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqq_VTQkeLFF"
      },
      "source": [
        "The larger window size seems to be forming some significantly better clusters of related terms. For instance, you can see one that relates specifically to the justices that neither of the others picked up on."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tutorial Three (Python): Introduction to Word Embeddings",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "988d93519b48dc480e47ace575b6155f797d463140a0ef2107140c89163106e1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
